````markdown
# 🧠 모델 관리와 모델 평가

## **1. 모델 관리의 필요성**

모델 관리란 머신러닝 모델의 **생성, 추적, 저장, 배포 전 과정을 체계적으로 관리**하는 것을 의미합니다.  
이는 단순한 성능 비교를 넘어, 모델의 **투명성**, **재현성**, **협업 효율성**을 높이는 핵심 단계입니다.

- **모델 다양성**: 여러 버전의 모델이 존재하며, 각각의 데이터셋·하이퍼파라미터·성능을 구분해 관리해야 함.  
- **투명성과 신뢰성**: 모델이 어떤 데이터와 코드로 만들어졌는지 기록함으로써 이해관계자 간 신뢰 확보.  
- **운영 효율성**: 동일 모델을 여러 환경(Local, Staging, Production)에서 안정적으로 재현 가능.  
- **협업 강화**: 데이터 사이언티스트, MLOps 엔지니어, 기획자 등 다양한 역할 간 정보 공유 용이.

> 💡 **비유**:  
> 여러 번의 실험을 통해 가장 맛있는 ‘레시피(모델)’를 찾고,  
> 그 과정과 결과를 기록해 “다시 같은 맛을 낼 수 있도록” 관리하는 것과 같다.

---

## **2. 모델 관리의 기본 요소**

1. **모델 메타데이터 (Metadata)**  
   - 생성일, 사용한 데이터셋 버전, 학습 환경, 하이퍼파라미터, 평가 지표 등을 포함.  
   - 모델이 “언제, 어떻게, 어떤 데이터로 만들어졌는가”를 설명하는 정보.

2. **모델 아티팩트 (Artifact)**  
   - 학습된 모델 결과물 자체 (`.pkl`, `.pt`, `.joblib` 등).  
   - 예측에 직접 활용되는 파일이므로 백업 및 버전 관리가 필수.

3. **Feature / Data 버전 관리**  
   - 데이터의 라벨링 변경, 전처리 파이프라인 업데이트 등에 따라 Feature 버전이 달라질 수 있음.  
   - 동일 모델이라도 입력 데이터 버전에 따라 성능이 달라질 수 있으므로 함께 관리해야 함.

---

## **3. MLflow 소개**

**MLflow**는 모델의 생애주기를 추적하고, 버전별 실험을 관리하는 **오픈소스 MLOps 플랫폼**입니다.  
모델 개발, 비교, 배포까지의 과정을 표준화하여 재현성과 협업성을 강화합니다.

### **3.1 MLflow 핵심 기능**

- **Tracking**  
  - 실험(Experiment) 단위로 하이퍼파라미터, 메트릭, 로그, 모델 파일을 기록.  
  - UI를 통해 여러 실험 결과를 시각적으로 비교 가능.

- **Model Registry**  
  - 모델 저장소. 각 모델은 자동으로 버전(`v1`, `v2`, …)이 관리됨.  
  - 승인 상태(“Staging”, “Production”) 설정으로 운영 환경 제어 가능.  
  - 팀 간 모델 공유 및 재활용이 용이.

- **Projects**  
  - 모델 훈련 코드를 표준화된 형태로 패키징하여 재현 가능한 실행 환경 제공.  
  - MLProject 파일을 통해 실행 방법과 의존성 명시.

- **Model Serving**  
  - 등록된 모델을 REST API 형태로 바로 서빙 가능.  
  - 별도 Docker 세팅 없이도 빠른 프로토타이핑 가능.

---

## **4. MLflow 실험 구조 요약**

```mermaid
graph TD
    A[Experiment 생성] --> B[Run 실행 (모델 훈련)]
    B --> C[Tracking 서버에 로그 기록]
    C --> D[Model Registry 등록]
    D --> E[Production 환경으로 Promote]
````

> 📁 `Experiment` : 프로젝트 단위 실험
> 📄 `Run` : 1회 코드 실행 (학습)
> 🧩 `Artifact` : 학습 결과물
> 🔄 `Registry` : 모델 버전 관리 및 배포 단계 제어

---

## **5. MLflow의 자동 로깅 (Autolog)**

* **`mlflow.autolog()`**

  * 학습 시 사용한 파라미터, 메트릭, 모델 파일 등을 자동으로 기록.
  * 수동 로깅보다 간결하고 재현성이 높음.
  * 단, 모든 프레임워크(Pytorch 등)를 완벽히 지원하지는 않음.

* **지원 프레임워크 예시:**
  Scikit-learn, XGBoost, LightGBM, TensorFlow, Pytorch Lightning 등.

> ⚠️ 주의:
> Pytorch 기본 모듈(`torch.nn.Module`)은 지원되지 않음.

---

## **6. MLflow Parameter 관리**

* `MLProject` 파일 내에서 파라미터를 정의하여 실험 실행 시 값 변경 가능.
* 명령어 예시:

  ```bash
  mlflow run logistic_regression_with_autolog_and_params \
      -P solver="saga" -P penalty="elasticnet" -P l1_ratio=0.03 \
      --experiment-name my-first-experiment --env-manager=local
  ```
* 여러 실험을 동시에 비교하여 최적의 하이퍼파라미터 조합을 시각화할 수 있음.

---

## **7. MLflow를 통한 협업**

* **데이터 사이언티스트**는 실험을 반복하며 모델 성능을 개선.
* **MLOps 엔지니어**는 Model Registry를 통해 운영 환경 배포를 관리.
* 모든 Run은 검색(`query`) 및 다운로드가 가능하여 재현성과 협업 효율이 높음.
* `mlflow.search_runs()`와 같은 API로 조건 검색 가능.

---

## **8. 모델 평가**

### **8.1 오프라인 모델 평가**

* **정의:**
  이미 수집된 고정 데이터셋으로 모델의 일반화 성능을 평가하는 방식.
* **방법:**

  * **Hold-out Validation:** 훈련/테스트 세트로 나누어 성능 평가.
  * **K-Fold Cross Validation:** 데이터를 K개의 부분으로 나눠 순환하며 평가.
  * **Bootstrap Resampling:** 중복 추출을 통해 분산을 고려한 성능 평가.

> 장점: 간단하고 빠름
> 단점: 실시간 변화 대응 어려움

---

### **8.2 온라인 모델 평가**

* **정의:**
  실시간 환경에서 모델의 예측 결과를 실제 사용자 반응과 비교하여 성능 평가.
* **기법:**

  * **A/B Test:**
    두 버전의 모델에 동일 트래픽을 분할 전송 후 성능 차이 비교.
    (단, 통계적 유의성 확보까지 시간이 오래 걸림)
  * **Canary Test:**
    새로운 모델에 일부 트래픽만 노출시켜 이상 유무를 점검.
    안전한 배포 단계 검증에 적합.
  * **Shadow Test:**
    실시간 트래픽을 복제하여 새 모델에 적용하되, 사용자에게 노출하지 않음.
    실제 데이터 기반 안정성 검증에 효과적.

> 💬 Canary와 Shadow는 운영 환경 안정성을 유지하며 리스크를 최소화하는 실전형 전략.

---

### **8.3 End-to-End 평가**

* 오프라인 → 온라인 평가를 반복하면서 모델을 지속적으로 개선.
* QA(품질 보증) 패턴을 활용하여 서비스 품질을 보장.
* 실제 운영 데이터와 사용자의 피드백을 통해 성능을 주기적으로 검증.

---

## **9. MLflow를 통한 모델 관리의 장점**

| 구분          | 내용                                 |
| ----------- | ---------------------------------- |
| **버전 관리**   | 모델의 모든 버전과 이력을 추적 가능               |
| **협업 환경**   | 여러 사용자가 동일한 실험 환경에서 공동 작업 가능       |
| **자동화된 로깅** | Autolog 기능으로 파라미터, 메트릭, 아티팩트 자동 저장 |
| **재현성 보장**  | MLProject를 통한 실행 환경 통일             |
| **통합 관리**   | 모델 개발 → 실험 → 등록 → 서빙까지 일원화된 관리 구조  |

---

## **10. 전체 구조 요약**

```mermaid
graph TD
    A[모델 개발 및 학습] --> B[MLflow Tracking - 로그 기록]
    B --> C[Model Registry - 버전 관리]
    C --> D[MLflow Serving - API 배포]
    D --> E[오프라인/온라인 평가 - 성능 모니터링]
    E --> F[최적 모델 Production 반영]
```

---

> ✅ **핵심 요약:**
> 모델 관리는 단순히 모델 파일을 저장하는 것을 넘어,
> **“모델의 생애주기를 추적하고, 협업과 품질을 보장하는 과정”**이다.
> MLflow와 같은 도구를 통해 자동화된 버전 관리와 평가 체계를 갖추면
> **재현성 + 신뢰성 + 효율성**을 모두 달성할 수 있다.

```